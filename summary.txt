1. Introduction to Reinforcement Learning (15 pages)
	Foundations in Behaviorist Psychology
		Introduction to behaviorism (focus on B.F. Skinner)
		Concept of operant conditioning
		Parallels between animal learning and RL
	Reinforcement Learning as a Model of Human Learning
		Environment and reward structure in human learning
		Comparison of RL to human task learning processes
		Concept of reward as gain (positive reinforcement) or punishment (negative reinforcement)
	Introduction to Reinforcement Learning in AI
		Definition of RL in the context of machine learning
		Key components: Agent, Environment, State, Action, Reward
	The RL Process
		Agent-environment interaction
		Exploration vs. exploitation dilemma
	Brief Overview of RL Approaches
		Value-based vs. policy-based methods
		Mention of common algorithms (e.g., Q-learning)
	Thesis Objectives
		Research questions
		Overview of RL models to be explored
2. Algorithms in Reinforcement Learning (10 pages)
	Here I'll detail the specific algorithms we've used in the research. This could include explanations of how they work, their strengths and weaknesses, and why we chose them for Ultimate Tic Tac Toe

3. Ultimate Tic Tac Toe rules and related works (5 pages)
	Introduction to Ultimate Tic Tac Toe
	Game Rules and Complexity
	Challenges in Strategy and Decision-Making

4. Game Theory (4 pages)
	Take inspiration from Alberto's thesis
	Relevant game theory concepts that apply to Ultimate Tic Tac Toe
	How game theory informs the approach to the problem

5. Experimental Setup and Results (20 pages)
	1. Introduction to the Experimental Chapter
		Brief overview of what this chapter covers.
		Goals of the experiments (e.g., identifying the best-performing agent).
	2. The Ultimate Tic Tac Toe Environment
		Environment Description:
		Explain the rules and mechanics of Ultimate Tic Tac Toe briefly.
		Highlight how the environment was coded (e.g., state representation, available actions, reward structure).
		Design Decisions:
		Discuss the design choices made (e.g., handling invalid moves, game termination conditions).
		Testing Environment:
		Describe the evaluation setup (e.g., hardware, software, and libraries used).
	3. Experimentation with RL Agents
		For each agent (DQN, DDQN, PPO, etc.), include:
		Agent Description:
		Briefly explain the agent’s architecture and learning algorithm.
		Training Details:
		Hyperparameters (e.g., learning rate, γ, batch size, exploration strategy).
		Training duration and computational cost.
		Results and Observations:
		Win/draw/loss rates against a random agent or other benchmarks.
		Any noteworthy behaviors or challenges during training (e.g., overfitting, instability).
	4. Head-to-Head Comparisons
		1v1 Round-Robin Tournament:
		Explain the methodology of the tournament (e.g., how games were scheduled, scoring system).
		Present results in a clear format, such as a table or matrix showing win rates for each pair of agents.
		Analysis:
		Discuss which agents performed the best and why.
		Highlight differences in playing styles or strategies adopted by the agents.
	5. Focus on the Best Agent
		Additional Training:
		Describe any fine-tuning or additional experiments performed with the best agent.
		Performance Evaluation:
		Analyze the best agent’s performance against advanced opponents (e.g., best response agents).
		Present key statistics, visualizations, or qualitative insights (e.g., how the agent handles specific board configurations).
	6. Summary of Findings
		Recap the results of the experiments.
		Highlight key insights and lessons learned (e.g., why some agents underperformed, what worked well).

7. Future works (1-2 pages)
	Potential improvements or extensions to the work
	Open questions and challenges in the field

